{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Neural Networks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qotzdSyq6lh_"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZdh3k9L7O-E"
      },
      "source": [
        "**INITIALIZING THE PARAMETERS RANDOMLY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9BXSSjg7XvJ"
      },
      "source": [
        "def initialize_parameters(layer_dimensions):\r\n",
        "  L = len(layer_dimensions) # total number of layers in the network\r\n",
        "  parameters = {}\r\n",
        "  for i in range(1, L):\r\n",
        "    parameters[\"W\" + i] = np.random.randn(layer_dimensions[i], layer_dimensions[i - 1]) * 0.01\r\n",
        "    parameters[\"b\" + i ] =  np.zeros((layer_dimensions[i], 1))\r\n",
        "  return parameters"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfUIt3n38bws"
      },
      "source": [
        "**ACTIVATION FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ1hkfTC8eX5"
      },
      "source": [
        "def ReLU(z):\r\n",
        "  cache = z\r\n",
        "  return np.maximum(0, z), cache"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3v3xkALJgIs"
      },
      "source": [
        "def relu_backward(dA, cache):\r\n",
        "    Z = cache\r\n",
        "    dZ = np.array(dA, copy=True) \r\n",
        "    dZ[Z <= 0] = 0\r\n",
        "    return dZ\r\n",
        "\r\n",
        "def sigmoid_backward(dA, cache):\r\n",
        "    Z = cache\r\n",
        "    s = 1/(1+np.exp(-Z))\r\n",
        "    dZ = dA * s * (1-s)\r\n",
        "    return dZ"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozZVJnql8s_o"
      },
      "source": [
        "**FORWARD PROPAGATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pZpolZL8xJp"
      },
      "source": [
        "def linear_forward(activation, weight, bias):\r\n",
        "  return np.dot(activation, weight) + bias, (activation, weight, bias) # cache"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOlsXVoA9f5z"
      },
      "source": [
        "def activation_forward(previous_activation, weight, bias, function):\r\n",
        "  Z, forward_cache = linear_forward(previous_activation, weight, bias)\r\n",
        "  if function == \"sigmoid\":\r\n",
        "     current_activation, activation_cache = np.sigmoid(Z)\r\n",
        "     cache = (forward_cache, activation_cache)\r\n",
        "  else:\r\n",
        "    current_activation, activation_cache = ReLU(Z)\r\n",
        "    cache = (forward_cache, activation_cache)\r\n",
        "  return current_activation, cache"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtxRm6Ev-bZ1"
      },
      "source": [
        "**FORWARD PROPAGATION MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-82JbOvS-ht0"
      },
      "source": [
        "def forward_propogation(inputs, parameters):\r\n",
        "   caches = []\r\n",
        "   A = X\r\n",
        "   L = len(parameters) // 2\r\n",
        "   for i in range(L):\r\n",
        "      A, cache = activation_forward(A_prev,    parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\r\n",
        "      caches.append(cache)\r\n",
        "   AL, cache = activation_forward(A, parameters['W' + str(L)],  parameters['b' + str(L)],  'sigmoid')\r\n",
        "   caches.append(cache)\r\n",
        "   return AL, caches"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMC0t6DxAt60"
      },
      "source": [
        "**CROSS ENTROPY COST FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fgGfLpkBPab"
      },
      "source": [
        "def compute_cost(y_hat, outputs, parameters):\r\n",
        "  number_of_examples = outputs.shape[0]\r\n",
        "  cost = (-1 / number_of_examples) * np.sum(np.multiply(np.log(y_hat), outputs) + np.multiply((1 -outputs), np.log(1 -y_hat)))\r\n",
        "  cost = np.squeeze(cost)\r\n",
        "  return cost"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7gKO1QSCMMQ"
      },
      "source": [
        "**BACKWARD PROPOGATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXdpJeArCPLr"
      },
      "source": [
        "def linear_backward(dZ, cache):\r\n",
        "    A_prev, W, b = cache\r\n",
        "    m = A_prev.shape[1]\r\n",
        "    dW = np.dot(dZ, cache[0].T) / m\r\n",
        "    db =np.sum(dZ, axis=1, keepdims=True) / m\r\n",
        "    dA_prev = np.dot(cache[1].T, dZ)\r\n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz_HcU70Hs8K"
      },
      "source": [
        "def activation_backward(dA, cache, function):\r\n",
        "     linear_cache, activation_cache = cache\r\n",
        "     if activation == \"relu\":\r\n",
        "        dZ = relu_backward(dA, activation_cache)       \r\n",
        "     else:\r\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\r\n",
        "     dA_prev, dW, db = linear_backward(dZ, linear_cache)\r\n",
        "     return dA_prev, dW, db"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUpS6mIkJzFI"
      },
      "source": [
        "**BACKWARD PROPOGATION MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig5FaoinJ1pv"
      },
      "source": [
        "def backward_propogation(AL, Y, caches):\r\n",
        "    grads = {}\r\n",
        "    L = len(caches) \r\n",
        "    m = AL.shape[1]\r\n",
        "    Y = Y.reshape(AL.shape) # Y is the same shape as AL\r\n",
        "    \r\n",
        "    dAL = dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\r\n",
        "    current_cache = caches[-1]\r\n",
        "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(sigmoid_backward(dAL,  current_cache[1]),  current_cache[0])\r\n",
        "    \r\n",
        "    for l in reversed(range(L-1)):\r\n",
        "        current_cache = caches[l]\r\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_backward(sigmoid_backward(dAL, current_cache[1]), current_cache[0])\r\n",
        "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\r\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\r\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\r\n",
        "    return grads"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0ydP2GGKRfR"
      },
      "source": [
        "**UPDATING THE PARAMETERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDhiy6w2KUUP"
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\r\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\r\n",
        "    for l in range(L):\r\n",
        "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\r\n",
        "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]       \r\n",
        "    return parameters"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn_KbVTVM07a"
      },
      "source": [
        "**TRAINING THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkZ6qgizM3Ov"
      },
      "source": [
        "def training_model(layer_dimensions, inputs, outputs, learning_rate, num_iterations):\r\n",
        "    costs = []         \r\n",
        "    parameters = initialize_parameters_deep(layers_dimensions) # initialize paramters\r\n",
        "    for i in range(0, num_iterations): # gradient descent\r\n",
        "        AL, caches = forward_propogation(inputs, parameters) # apply forward propogation\r\n",
        "        cost = compute_cost(AL, outputs) # compute cost\r\n",
        "        grads = backward_propogation(AL, outputs, caches) # apply backward propogation\r\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\r\n",
        "        if  i % 100 == 0:\r\n",
        "          costs.append(cost)     \r\n",
        "    return parameters"
      ],
      "execution_count": 46,
      "outputs": []
    }
  ]
}